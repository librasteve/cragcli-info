{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1078ba4fc9a3b917",
   "metadata": {},
   "source": [
    "gonna try with Python Beautiful Soup first\n",
    "\n",
    "python set up\n",
    "\n",
    "```bash\n",
    "deactivate  (when in .venv)\n",
    "\n",
    "rm -rf .venv\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "which python3\n",
    ".venv/bin/python3 -m pip install requests bs4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db729ae1500198d9",
   "metadata": {},
   "source": [
    "note prior work on hn-comments.sh\n",
    "\n",
    "```\n",
    "rak 'raku' --type=equal --json-per-file hn-comments-raku.json > rak-out.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be63c812f5ea42cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T14:09:24.665377Z",
     "start_time": "2025-12-29T14:09:23.197758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Beautiful Soup does not have built-in plugins for services like Google search or Algolia. However, you can use Beautiful Soup in conjunction with other libraries or APIs to achieve the same functionality. For example, you could use Beautiful Soup to scrape search results from Google and then use a library like requests or urllib to make the actual search queries. Similarly, you could use Beautiful Soup to scrape data from a website and then use an API like Algolia's search API to search and filter that data."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "does beatiful soup have plugins for eg #% openaigoogle search or algolia?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c0bf7d0986fc02e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T14:12:17.373125Z",
     "start_time": "2025-12-29T14:12:16.057289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Algolia is a search-as-a-service platform that provides businesses with the tools they need to create fast, relevant, and customizable search experiences for their users. With its powerful search algorithm and easy-to-use API, Algolia helps companies improve the discoverability of their content and products, leading to increased user engagement and conversion rates."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#% openai\n",
    "what is algolia (one para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e8151d7d09cbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b67bc5e1004b578",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T13:41:12.719984Z",
     "start_time": "2025-12-29T13:40:52.406625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "To automate scraping comments about \"raku programming language\" from Google search results for the past week, including extracting sentences that mention \"raku,\" author names, and links, you can use a combination of tools and APIs. Here’s a high-level approach:\n",
       "\n",
       "### Important Notes\n",
       "- Scraping Google directly is against Google's Terms of Service and can get your IP blocked.\n",
       "- Consider using Google Custom Search JSON API, which is a legitimate way to query Google search programmatically.\n",
       "- For author and comment details, results might vary since Google search snippets don’t always show author info.\n",
       "- Alternatively, you can scrape specific forums, social media, or dedicated comment platforms where \"raku\" is discussed.\n",
       "\n",
       "---\n",
       "\n",
       "### Step 1: Use Google Custom Search API\n",
       "Google Custom Search JSON API allows programmatic search queries with filters like date range.\n",
       "\n",
       "- Set up a Custom Search Engine (CSE) restricted to the web.\n",
       "- Use the `q` parameter for your search term.\n",
       "- Use `sort=date` to get recent results.\n",
       "- Use `dateRestrict=w[1]` to restrict to the last week.\n",
       "\n",
       "Example API call format:\n",
       "\n",
       "```\n",
       "https://www.googleapis.com/customsearch/v1?q=raku+programming+language&sort=date&dateRestrict=w1&key=YOUR_API_KEY&cx=YOUR_CSE_ID\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### Step 2: Parse the Search Results\n",
       "\n",
       "The API returns JSON with fields like:\n",
       "\n",
       "- `title`\n",
       "- `link`\n",
       "- `snippet` (summary)\n",
       "\n",
       "Example: extract sentences from `snippet` containing \"raku\".\n",
       "\n",
       "---\n",
       "\n",
       "### Step 3: (Optional) Extract Author Information\n",
       "\n",
       "Google search snippets do not usually show author names. To get author info:\n",
       "\n",
       "- Follow the `link` to the actual page.\n",
       "- Scrape the page for author metadata (e.g., `<meta name=\"author\">`, or author elements).\n",
       "- This requires extra scraping and depends on the site's structure.\n",
       "\n",
       "---\n",
       "\n",
       "### Step 4: Example Python Script\n",
       "\n",
       "```python\n",
       "import requests\n",
       "import re\n",
       "\n",
       "API_KEY = 'YOUR_API_KEY'\n",
       "CSE_ID = 'YOUR_CSE_ID'\n",
       "QUERY = 'raku programming language'\n",
       "NUM_RESULTS = 10\n",
       "\n",
       "def google_search(query, api_key, cse_id, num=10):\n",
       "    url = 'https://www.googleapis.com/customsearch/v1'\n",
       "    params = {\n",
       "        'key': api_key,\n",
       "        'cx': cse_id,\n",
       "        'q': query,\n",
       "        'num': num,\n",
       "        'sort': 'date',\n",
       "        'dateRestrict': 'w1',\n",
       "    }\n",
       "    response = requests.get(url, params=params)\n",
       "    return response.json()\n",
       "\n",
       "def extract_raku_sentences(snippet):\n",
       "    # find sentences containing 'raku'\n",
       "    sentences = re.split(r'(?<=[.!?]) +', snippet)\n",
       "    raku_sentences = [s for s in sentences if 'raku' in s.lower()]\n",
       "    return raku_sentences\n",
       "\n",
       "def main():\n",
       "    results = google_search(QUERY, API_KEY, CSE_ID, NUM_RESULTS)\n",
       "    for item in results.get('items', []):\n",
       "        link = item.get('link')\n",
       "        snippet = item.get('snippet', '')\n",
       "        sentences = extract_raku_sentences(snippet)\n",
       "        print(f'Link: {link}')\n",
       "        print('Sentences mentioning \"raku\":')\n",
       "        for s in sentences:\n",
       "            print(f' - {s}')\n",
       "        print('Author info may require scraping the page separately.')\n",
       "        print('---')\n",
       "\n",
       "if __name__ == '__main__':\n",
       "    main()\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### Step 5: Scraping Author from the Linked Page (Optional)\n",
       "\n",
       "You can use libraries like `BeautifulSoup` to scrape author data from each linked page.\n",
       "\n",
       "Example:\n",
       "\n",
       "```python\n",
       "from bs4 import BeautifulSoup\n",
       "\n",
       "def get_author_from_page(url):\n",
       "    resp = requests.get(url)\n",
       "    if resp.status_code == 200:\n",
       "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
       "        author = soup.find('meta', attrs={'name': 'author'})\n",
       "        if author and author.get('content'):\n",
       "            return author['content']\n",
       "        # Try other selectors or patterns as needed\n",
       "    return None\n",
       "```\n",
       "\n",
       "Call this function for each link to attempt to get author info.\n",
       "\n",
       "---\n",
       "\n",
       "### Summary\n",
       "\n",
       "- Use Google Custom Search API for automated querying with date restriction.\n",
       "- Parse snippets to extract sentences mentioning \"raku\".\n",
       "- For author names, scrape individual pages.\n",
       "- Respect site scraping policies and terms of service.\n",
       "- Consider also searching specialized platforms (forums, Reddit, StackOverflow) for richer comment data.\n",
       "\n",
       "If you want, I can help you create a full script or guide you setting up the API and scraping."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%% chat-t1, prompt = «i currently have a manual process to get a list of comments about the \"raku programming language\" from google.com from the last week:- i enter that search term in the google search box and then select tools > past week ... how can i automate this to scrape the actual sentence that mentions raku (eg from the summary), the name of the author and a link to the item»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57eeb2d2ed87de66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T14:01:54.895749Z",
     "start_time": "2025-12-29T14:01:34.611986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Great! I'll guide you step-by-step to set up and run a Python script that:\n",
       "\n",
       "- Queries Google search results via the Google Custom Search JSON API.\n",
       "- Restricts results to the past week.\n",
       "- Extracts sentences mentioning \"raku\".\n",
       "- Attempts to scrape author metadata from the linked pages.\n",
       "\n",
       "---\n",
       "\n",
       "# Step 1: Set up Google Custom Search API\n",
       "\n",
       "1. **Create a Google Cloud Project & enable Custom Search API:**\n",
       "\n",
       "- Go to [Google Cloud Console](https://console.cloud.google.com).\n",
       "- Create a new project (or select an existing one).\n",
       "- Go to **APIs & Services > Library**, find **Custom Search API**, and enable it.\n",
       "\n",
       "2. **Create API Key:**\n",
       "\n",
       "- In **APIs & Services > Credentials**, click **Create Credentials > API key**.\n",
       "- Copy your API key.\n",
       "\n",
       "3. **Create a Custom Search Engine (CSE):**\n",
       "\n",
       "- Go to [https://cse.google.com/cse/all](https://cse.google.com/cse/all).\n",
       "- Click **Add**.\n",
       "- In \"Sites to search\" enter `www.google.com` or leave blank and select \"Search the entire web\" in **Setup > Sites to search > Search the entire web but emphasize included sites**.\n",
       "- Create the CSE.\n",
       "- In the control panel, get the **Search Engine ID** (cx parameter).\n",
       "- Enable Image Search if you want (optional).\n",
       "\n",
       "---\n",
       "\n",
       "# Step 2: Install necessary Python packages\n",
       "\n",
       "You'll need:\n",
       "\n",
       "- `requests` for HTTP requests\n",
       "- `beautifulsoup4` for HTML parsing\n",
       "\n",
       "Install with pip:\n",
       "\n",
       "```bash\n",
       "pip install requests beautifulsoup4\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "# Step 3: Python Script\n",
       "\n",
       "```python\n",
       "import requests\n",
       "import re\n",
       "from bs4 import BeautifulSoup\n",
       "import time\n",
       "\n",
       "API_KEY = 'YOUR_API_KEY'          # Replace with your API key\n",
       "CSE_ID = 'YOUR_CSE_ID'            # Replace with your Search Engine ID\n",
       "QUERY = 'raku programming language'\n",
       "NUM_RESULTS = 10                  # Number of results per request (max 10 per Google API)\n",
       "DELAY_BETWEEN_REQUESTS = 1       # To avoid rate limits\n",
       "\n",
       "def google_search(query, api_key, cse_id, num=10, start=1):\n",
       "    url = 'https://www.googleapis.com/customsearch/v1'\n",
       "    params = {\n",
       "        'key': api_key,\n",
       "        'cx': cse_id,\n",
       "        'q': query,\n",
       "        'num': num,\n",
       "        'start': start,\n",
       "        'sort': 'date',\n",
       "        'dateRestrict': 'w1',  # restrict to last week\n",
       "    }\n",
       "    response = requests.get(url, params=params)\n",
       "    response.raise_for_status()\n",
       "    return response.json()\n",
       "\n",
       "def extract_raku_sentences(text):\n",
       "    # Split snippet into sentences and return only those mentioning 'raku'\n",
       "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
       "    return [s for s in sentences if 'raku' in s.lower()]\n",
       "\n",
       "def get_author_from_page(url):\n",
       "    try:\n",
       "        headers = {\n",
       "            \"User-Agent\": \"Mozilla/5.0 (compatible; RakuScraper/1.0)\"\n",
       "        }\n",
       "        resp = requests.get(url, headers=headers, timeout=10)\n",
       "        resp.raise_for_status()\n",
       "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
       "        # Try meta tag 'author'\n",
       "        author_meta = soup.find('meta', attrs={'name': 'author'})\n",
       "        if author_meta and author_meta.get('content'):\n",
       "            return author_meta['content'].strip()\n",
       "\n",
       "        # Try meta tag 'article:author' (common in OpenGraph)\n",
       "        og_author = soup.find('meta', attrs={'property': 'article:author'})\n",
       "        if og_author and og_author.get('content'):\n",
       "            return og_author['content'].strip()\n",
       "\n",
       "        # Try common author selectors\n",
       "        for candidate in ['.author', '.byline', '.post-author']:\n",
       "            el = soup.select_one(candidate)\n",
       "            if el:\n",
       "                return el.get_text(strip=True)\n",
       "\n",
       "    except Exception as e:\n",
       "        print(f\"Error fetching author from {url}: {e}\")\n",
       "    return \"Unknown\"\n",
       "\n",
       "def main():\n",
       "    all_results = []\n",
       "    start_index = 1\n",
       "\n",
       "    # Google API allows max 100 results by paging with &start\n",
       "    while start_index <= NUM_RESULTS:\n",
       "        results = google_search(QUERY, API_KEY, CSE_ID, num=min(10, NUM_RESULTS - start_index +1), start=start_index)\n",
       "        items = results.get('items', [])\n",
       "        if not items:\n",
       "            break\n",
       "\n",
       "        for item in items:\n",
       "            link = item.get('link')\n",
       "            snippet = item.get('snippet', '')\n",
       "            sentences = extract_raku_sentences(snippet)\n",
       "            author = get_author_from_page(link) if link else \"Unknown\"\n",
       "\n",
       "            print(f\"Link: {link}\")\n",
       "            print(f\"Author: {author}\")\n",
       "            print(\"Sentences mentioning 'raku':\")\n",
       "            for s in sentences:\n",
       "                print(f\"  - {s}\")\n",
       "            print(\"-\" * 60)\n",
       "\n",
       "        start_index += len(items)\n",
       "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
       "\n",
       "if __name__ == '__main__':\n",
       "    main()\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "# Step 4: Replace placeholders and run\n",
       "\n",
       "- Replace `'YOUR_API_KEY'` and `'YOUR_CSE_ID'` with the values you obtained.\n",
       "- You can adjust `NUM_RESULTS` (max 100) and `QUERY` as needed.\n",
       "- Run the script: `python your_script.py`\n",
       "\n",
       "---\n",
       "\n",
       "# Notes & Tips\n",
       "\n",
       "- The author extraction is heuristic; results will vary per site.\n",
       "- Google API has daily quotas (like 100 queries/day free).\n",
       "- To get more comments, you might want to parse dedicated comment platforms or forums.\n",
       "- Respect robots.txt and terms of service when scraping linked pages.\n",
       "- Add error handling and logging as needed for production use.\n",
       "\n",
       "---\n",
       "\n",
       "If you want, I can help you customize this script further!"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%% chat-t1\n",
    "yes please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "450b7bb364431a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T14:04:49.533023Z",
     "start_time": "2025-12-29T14:04:36.276483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sure! Here's how to get your **API Key** and **Custom Search Engine ID (CSE ID)** step-by-step:\n",
       "\n",
       "---\n",
       "\n",
       "## How to get **YOUR_API_KEY**\n",
       "\n",
       "1. **Go to Google Cloud Console:**\n",
       "\n",
       "   Open [https://console.cloud.google.com](https://console.cloud.google.com) and sign in with your Google account.\n",
       "\n",
       "2. **Create or select a project:**\n",
       "\n",
       "   - Click the project dropdown at the top left.\n",
       "   - Click **New Project**.\n",
       "   - Enter a project name (e.g., \"RakuSearchAutomation\").\n",
       "   - Click **Create**.\n",
       "   - Select the newly created project.\n",
       "\n",
       "3. **Enable the Custom Search API:**\n",
       "\n",
       "   - In the left sidebar, click **APIs & Services > Library**.\n",
       "   - Search for **Custom Search API**.\n",
       "   - Click it, then click **Enable**.\n",
       "\n",
       "4. **Create API Credentials:**\n",
       "\n",
       "   - In the sidebar, go to **APIs & Services > Credentials**.\n",
       "   - Click **Create Credentials > API key**.\n",
       "   - An API key will be generated and shown in a popup.\n",
       "   - Copy this API key — this is your `YOUR_API_KEY`.\n",
       "\n",
       "---\n",
       "\n",
       "## How to get **YOUR_CSE_ID** (Custom Search Engine ID)\n",
       "\n",
       "1. **Go to Custom Search Engine dashboard:**\n",
       "\n",
       "   Visit [https://cse.google.com/cse/all](https://cse.google.com/cse/all).\n",
       "\n",
       "2. **Create a new Search Engine:**\n",
       "\n",
       "   - Click **Add**.\n",
       "   - In the “Sites to search” field, enter: `www.google.com` (or any site for testing).\n",
       "   - Give it a name (e.g., \"RakuSearch\").\n",
       "   - Click **Create**.\n",
       "\n",
       "3. **Allow searching the entire web:**\n",
       "\n",
       "   - After creation, click **Control Panel** for your new search engine.\n",
       "   - Find the **Sites to search** section.\n",
       "   - Click **Edit**.\n",
       "   - Choose **Search the entire web but emphasize included sites**.\n",
       "   - Save changes.\n",
       "\n",
       "4. **Find your Search Engine ID:**\n",
       "\n",
       "   - At the top of the Control Panel page, you will see **Search engine ID**.\n",
       "   - This is your `YOUR_CSE_ID`. Copy it.\n",
       "\n",
       "---\n",
       "\n",
       "## Summary\n",
       "\n",
       "- Use the **API Key** from Google Cloud Console.\n",
       "- Use the **Search Engine ID** from your Custom Search Engine control panel.\n",
       "\n",
       "Plug both into the Python script:\n",
       "\n",
       "```python\n",
       "API_KEY = 'YOUR_ACTUAL_API_KEY_HERE'\n",
       "CSE_ID = 'YOUR_ACTUAL_CSE_ID_HERE'\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "If you want, I can also guide you through testing a simple API call to verify your setup!"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%% chat-t1\n",
    "how do i get a YOUR_API_KEY'` and `'YOUR_CSE_ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f4e2edf3744308",
   "metadata": {},
   "outputs": [],
   "source": [
    "iamerejh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RakuChatbook",
   "language": "raku",
   "name": "raku"
  },
  "language_info": {
   "file_extension": ".raku",
   "mimetype": "text/x-raku",
   "name": "raku",
   "version": "6.d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
