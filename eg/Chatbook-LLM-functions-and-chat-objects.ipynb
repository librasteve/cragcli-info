{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM functions and chat objects\n",
    "\n",
    "Anton Antonov   \n",
    "[\"Jupyter::Chatbook\" Raku package at GitHub](https://github.com/antononcube/Raku-Jupyter-Chatbook)   \n",
    "August, September 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we show how Large Language Models (LLMs) functions and LLM chat objects can be created and used in a notebook. This notebook is a ***chatbook*** created with the Raku package [\"Jupyter::Chatbook\"](https://raku.land/zef:antononcube/Jupyter::Chatbook), [AAp1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A \"streamlined\" way to utilize LLMs is with the package[\"LLM::Functions\"](https://raku.land/zef:antononcube/LLM::Functions), [AA1, AAp2]. I.e. without using specific, dedicated packages for accessing LLMs like those of [\"WWW::OpenAI\"](https://raku.land/zef:antononcube/WWW::OpenAI), [AAp3], and [\"WWW::PaLM\"](https://raku.land/zef:antononcube/WWW::PaLM), [AAp4].\n",
    "\n",
    "Chatbooks load in their initialization phase the package \n",
    "[\"LLM::Functions\"](https://github.com/antononcube/Raku-LLM-Functions), [AAp2].\n",
    "Also, in the initialization phase are loaded the packages \n",
    "[\"Clipboard\"](https://github.com/antononcube/Raku-Clipboard), [AAp5],\n",
    "[\"Data::Translators\"](https://github.com/antononcube/Raku-Data-Translators), [AAp6],\n",
    "[\"Data::TypeSystem\"](https://github.com/antononcube/Raku-Data-Translators), [AAp7],\n",
    "[\"Text::Plot\"](https://github.com/antononcube/Raku-Text-Plot), [AAp8],\n",
    "and \n",
    "[\"Text::SubParsers\"](https://github.com/antononcube/Raku-Text-SubParsers), [AAp9], \n",
    "that can be used to post-process LLM outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** For LLM functions and chat objects the functionalities of [\"Jupyter::Chatbook\"](https://github.com/bduggan/raku-jupyter-kernel), [BDp1], suffice. In other words, the \"chatbook\" extensions provided by \"Jupyter::Chatbook\", [AAp1], are not needed.\n",
    "\n",
    "**Remark:** The LLM functions and chat objects are provided by the package [\"LLM::Functions\"](https://raku.land/zef:antononcube/LLM::Functions), [AA1, AAp2], which in turn uses the packages [\"WWW::OpenAI\"](https://raku.land/zef:antononcube/WWW::OpenAI), [AAp3], and [\"WWW::PaLM\"](https://raku.land/zef:antononcube/WWW::PaLM), [AAp4].\n",
    "\n",
    "**Remark:** The default API keys for the LLM functions and chat objects are taken from the Operating System (OS) environmental variables `OPENAI_API_KEY` and `PALM_API_KEY`. The api keys can also be specified using LLM evaluator and configuration options and objects; see [AA1, AAp2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ \n",
    "\n",
    "## LLM functions\n",
    "\n",
    "\n",
    "\n",
    "Here is an example of an LLM function that takes 3 arguments:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "my &f1 = llm-function({\"What is the $^a of $^b in $^c. Give numerical answer only.\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the LLM function defined above is \"concretized\" with argument values:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "&f1('GDP', 'California, USA', '2020')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call repeatedly `&f1` in order to Lake Mead levels: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "'level' X 'Lake Mead' X (1990...2010) ==> map({ $_.tail => &f1(|$_) }) ==> my @levels;"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extract numbers of the obtained LLM results and plot the levels over the corresponding years:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "@levels.map({ ($_.key, sub-parser('GeneralNumber', :drop).process($_.value).head) }) ==> text-list-plot(width=>60)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we defined another LLM function, and since the LLM function produces (or it is suppossed to produce) HTML code we use the magic spec `%% html`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%% html\n",
    "\n",
    "my &ftbl = llm-function({\"The HTML code of a random table with $^a rows and $^b columns is : \"}, e => 'ChatGPT');\n",
    "\n",
    "&ftbl(4,3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed examples of LLM workflows can be found in the article [\"Workflows with LLM functions\"](https://rakuforprediction.wordpress.com/2023/08/01/workflows-with-llm-functions/), [AA1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Number extraction from LLM responses\n",
    "\n",
    "Often LLM return larger number with comma delimiters between the digits. Here is an example: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "my $pop = llm-function()(\"What is the population of Niger?\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to extract the numbers from those responses is to use the token `<local-number>` provided by the package [\"Intl::Token::Number\"](https://raku.land/zef:guifa/Intl::Token::Number), [MSp1]:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "use Intl::Token::Number;\n",
    "\n",
    "$pop ~~ m:g/ <local-number> /"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a sub-parser from the packatge [\"Text::SubParsers\"](https://raku.land/zef:antononcube/Text::SubParsers), [AAp8], can be used:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sub-parser(Whatever).subparse($pop).raku"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## LLM chat objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM chat objects are used to provide context for LLM interactions with different messages.\n",
    "LLM chat objects keep track of the conversation history, so LLM can better understand the meaning of each message.\n",
    "\n",
    "For more involved examples of using LLM chat objects see the article [\"Number guessing games: PaLM vs ChatGPT\"](https://rakuforprediction.wordpress.com/2023/08/06/number-guessing-games-palm-vs-chatgpt/), [AA2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a chat object:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "my $chatObj = llm-chat(\n",
    "    conf=>'OpenAI',\n",
    "    prompt => 'You are Raku coding instructor. You are the best Raku programmer and you know Raku documentation very well. You answers are concise, mostly with code');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give the chat object a message, and specify the output to be in Markdown format using the magic spec `%% markdown`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%% markdown\n",
    "$chatObj.eval('How do you get the characters of a string?')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%% markdown\n",
    "$chatObj.eval('How do you make the original word from the characters in the previous example')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles\n",
    "\n",
    "[AA1] Anton Antonov,\n",
    "[\"Workflows with LLM functions\"](https://rakuforprediction.wordpress.com/2023/08/01/workflows-with-llm-functions/),\n",
    "(2023),\n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com).\n",
    "\n",
    "[AA2] Anton Antonov,\n",
    "[\"Number guessing games: PaLM vs ChatGPT\"](https://rakuforprediction.wordpress.com/2023/08/06/number-guessing-games-palm-vs-chatgpt/),\n",
    "(2023),\n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com).\n",
    "\n",
    "### Packages\n",
    "\n",
    "[AAp1] Anton Antonov,\n",
    "[Jupyter::Chatbook Raku package](https://github.com/antononcube/Raku-Jupyter-Chatbook),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp2] Anton Antonov,\n",
    "[LLM::Functions Raku package](https://github.com/antononcube/Raku-LLM-Functions),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp3] Anton Antonov,\n",
    "[WWW::OpenAI Raku package](https://github.com/antononcube/Raku-WWW-OpenAI),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp4] Anton Antonov,\n",
    "[WWW::PaLM Raku package](https://github.com/antononcube/Raku-WWW-PaLM),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp5] Anton Antonov,\n",
    "[Clipboard Raku package](https://github.com/antononcube/Raku-Clipboard),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp6] Anton Antonov,\n",
    "[Data::Translators Raku package](https://github.com/antononcube/Raku-Data-Translators),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp7] Anton Antonov,\n",
    "[Data::TypeSystem Raku package](https://github.com/antononcube/Raku-Data-TypeSystem),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp8] Anton Antonov,\n",
    "[Text::Plot Raku package](https://github.com/antononcube/Raku-Text-Plot),\n",
    "(2022),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp9] Anton Antonov,\n",
    "[Text::SubParsers Raku package](https://github.com/antononcube/Raku-Text-SubParsers),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[BDp1] Brian Duggan,\n",
    "[Jupyter:Kernel Raku package](https://github.com/bduggan/raku-jupyter-kernel),\n",
    "(2017-2023),\n",
    "[GitHub/bduggan](https://github.com/bduggan).\n",
    "\n",
    "[MSp1] Matthew Stuckwisch,\n",
    "[Intl::Token::Number Raku package](https://github.com/alabamenhu/IntlTokenNumber)\n",
    "(2021-2023),\n",
    "[GitHub/alabamenhu](https://github.com/alabamenhu)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RakuChatbook",
   "language": "raku",
   "name": "raku"
  },
  "language_info": {
   "file_extension": ".raku",
   "mimetype": "text/x-raku",
   "name": "raku",
   "version": "6.d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
